# Big Data with PySpark using Anaconda & Jupyter Notebook

Welcome to the Big Data with PySpark project repository! This project focuses on performing data analysis using PySpark within an Anaconda environment and Jupyter Notebook. You'll explore various tasks related to weather data analysis, covering topics from basic data manipulation to complex queries.

## Project Structure

This repository includes the following main components:

- **Jupyter Notebook (`bigdata.ipynb`):** Contains the PySpark code and commands used to perform the data analysis tasks.
- **Results File (`result.txt`):** A text file containing all the results information generated from the executed PySpark operations.

## Tasks Overview

The project consists of the following tasks, aimed at exploring weather data across different years:

1. **Hottest Day Analysis:** Identify the hottest day of each year, including the station code, name, and date.
2. **Coldest Day in January:** Determine the coldest day in January across all years (2010 - 2022), including station details and date.
3. **Precipitation Extremes in 2015:** Find the maximum and minimum precipitation events in 2015, with corresponding station information.
4. **Wind Gust Data Completeness for 2019:** Calculate the percentage of missing wind gust data for the year 2019.
5. **Temperature Statistics for 2020:** Analyze the mean, median, mode, and standard deviation of temperatures for each month of 2020.

## Data Access

The dataset required for this project is provided in the `data.zip` file within this repository. Please ensure to extract and utilize this data for performing the analysis tasks.

## Getting Started

To get started with the project, follow these steps:

1. Ensure you have Anaconda and Jupyter Notebook installed on your system.
2. Download the provided `data.zip` file and extract its contents.
3. Open the `bigdata.ipynb` notebook in Jupyter to view the tasks and start coding.
